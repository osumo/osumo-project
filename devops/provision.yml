---

- hosts: localhost
  connection: local
  become: false
  pre_tasks:
    - name: AWS credentials | parse
      aws_credentials:
        profile: sumo

  roles:
  - role: ec2-pod2
    name: sumo-testing
    region: us-east-1
    ssh_keys:
      main: "~/.ssh/sumo.pub"
    instances:
      "prod-web":
        ssh_key: main
        type: t2.medium
        image: ami-fce3c696
        volumes: [2, 10]
        security_groups: ["open"]
        ansible_groups: ["sumo", "production", "web"]
      "prod-db":
        ssh_key: main
        type: t2.medium
        image: ami-fce3c696
        volumes: [2, 30]
        security_groups: ["open"]
        ansible_groups: ["sumo", "production", "db"]
      "prod-worker":
        ssh_key: main
        type: t2.medium
        image: ami-fce3c696
        volumes: [2, 30]
        security_groups: ["open"]
        ansible_groups: ["sumo", "production", "worker"]

      "stage-web":
        ssh_key: main
        type: t2.micro
        image: ami-fce3c696
        volumes: [2, 10]
        security_groups: ["open"]
        ansible_groups: ["sumo", "staging", "web"]
      "stage-db":
        ssh_key: main
        type: t2.micro
        image: ami-fce3c696
        volumes: [2, 10]
        security_groups: ["open"]
        ansible_groups: ["sumo", "staging", "db"]
      "stage-worker":
        ssh_key: main
        type: t2.micro
        image: ami-fce3c696
        volumes: [2, 10]
        security_groups: ["open"]
        ansible_groups: ["sumo", "staging", "worker"]
    security_groups:
      open:
        - { flow: "sym", proto: "all", cidr_ip: "0.0.0.0/0" }

- hosts: sumo
  connection: local
  gather_facts: no
  tasks:
    - name: facts | set
      set_fact:
        is_web: "{{ inventory_hostname in groups.get('web') }}"
        is_db: "{{ inventory_hostname in groups.get('db') }}"
        is_worker: "{{ inventory_hostname in groups.get('worker') }}"
        is_prod: "{{ inventory_hostname in groups.get('production') }}"
        is_stage: "{{ inventory_hostname in groups.get('staging') }}"

    - name: facts | set
      set_fact:
        is_not_web: "{{ not (is_web|bool) }}"
        is_not_db: "{{ not (is_db|bool) }}"
        is_not_worker: "{{ not (is_worker|bool) }}"
        is_not_prod: "{{ not (is_prod|bool) }}"
        is_not_stage: "{{ not (is_stage|bool) }}"

    - name: groups | set
      group_by:
        key: >-
          {{ (is_db|bool) and ((is_prod|bool) and "pdb" or "sdb") or "x" }}

- hosts: sumo
  user: ubuntu
  become: true
  pre_tasks:
    - name: filesystems | format
      filesystem:
        fstype: ext4
        dev: "/dev/xvd{{ item }}"
      with_items: ["b", "c"]

    - name: filesystems | mount
      mount:
        fstype: ext4
        name: "{{ item.value }}"
        src: "/dev/xvd{{ item.key }}"
        state: mounted
      with_dict:
        b: /repo
        c: /opt

    - name: apt packages | install
      apt:
        name: "{{ item }}"
        state: present
        update_cache: true
      with_items:
        - git
        - python-apt
        - apt-transport-https
        - wget
        - python-virtualenv
        - python-dev

    - name: apt packages | install
      apt:
        name: "{{ item }}"
        state: present
        update_cache: true
      with_items:
        - libffi-dev
        - libxml2-dev
      when: is_not_db|bool

    - name: osumo-project | clone
      git_cache:
        repo: "git://github.com/osumo/osumo-project.git"
        dest: /opt/osumo-project
        version: demo-2016-04-08
        recursive: yes
        state: present
      register: osumo_repo
      when: is_not_db|bool

    - name: plugin | osumo | symlink
      file:
        src: "{{ osumo_repo.destination }}/osumo"
        dest: "{{ osumo_repo.destination }}/girder/plugins/osumo"
        state: link
      when: is_not_db|bool

    # Notice the "become: false" clause.  When creating tmp dirs locally, it
    # must be done as the current user because Ansible's fetch module will only
    # write locally as the current user.
    - name: tmp dir | create
      local_action: command mktemp -d --tmpdir=/tmp osumo.XXXXXXXXXX
      become: false
      register: tempdir

    - name: templates | fetch
      fetch:
        src: "{{ osumo_repo.destination }}/devops/templates/{{ item }}"
        dest: "{{ tempdir.stdout }}/{{ item }}"
        flat: yes
        fail_on_missing: yes
      with_items:
        - girder.local.cfg.j2
        - worker.local.cfg.j2
        - girder.bash.j2
        - worker.bash.j2
      when: is_not_db|bool

    - name: resonant | configure
      template:
        src: "{{ tempdir.stdout }}/{{ item.src }}"
        dest: "{{ osumo_repo.destination }}/{{ item.dest }}"
      with_items:
        - src: girder.local.cfg.j2
          dest: girder/girder/conf/girder.local.cfg
        - src: worker.local.cfg.j2
          dest: girder_worker/girder_worker/worker.local.cfg
        - src: girder.bash.j2
          dest: girder.bash
        - src: worker.bash.j2
          dest: worker.bash
      when: is_not_db|bool

    # NOTE(opadron): this was the only way I could get ansible to delete all the
    # temporary directories (OS X)
    - name: tmp dir | remove
      local_action: shell rm -rf /tmp/osumo.*
      become: false

    - name:  nodesource | apt | key | fetch
      apt_key:
        url: "https://deb.nodesource.com/gpgkey/nodesource.gpg.key"
        state: present
      when: is_web|bool

    - name: nodesource | apt | repository | add
      apt_repository:
        repo: "{{ item }} https://deb.nodesource.com/node_0.12 trusty main"
        state: present
      with_items:
        - deb
        - deb-src
      when: is_web|bool

    - name: nodejs | install
      apt:
        name: nodejs
        update_cache: yes
      when: is_web|bool

  roles:
    - role: rabbitmq
      state: started
      when: is_worker|bool

    - role: mongodb
      state: started
      when: is_db|bool

    - role: upstart
      name: girder
      user: root
      group: root
      description: Girder Data Management Platform -- Web Service
      command: >-
          bash -e "{{ osumo_repo.destination }}/girder.bash"
      when: is_web|bool

    - role: upstart
      name: girder_worker
      user: root
      group: root
      description: Girder Worker Execution Engine Service
      command: >-
          bash -e "{{ osumo_repo.destination }}/worker.bash"
      when: is_worker|bool

  post_tasks:
    - name: cran repository | key | fetch
      apt_key:
        keyserver: keyserver.ubuntu.com
        id: E084DAB9
        state: present
      when: is_worker|bool

    - name: cran repository | add
      apt_repository:
        repo: "deb https://cloud.r-project.org/bin/linux/ubuntu trusty/"
        state: present
      when: is_worker|bool

    - name: packages | install
      apt:
        name: "{{ item }}"
        state: present
        update_cache: true
      with_items:
      - libcurl4-gnutls-dev
      - r-base
      when: is_worker|bool

    - name: R packages | install
      command: >-
        Rscript --slave --no-save --no-restore-history -e
        "install.packages('{{ item }}', repos='http://cran.rstudio.com')"
      with_items:
      - shiny
      - jsonlite
      - pheatmap
      - survival
      # - devtools
      # - factoextra devtools::install_github("kassambara/factoextra")
      when: is_worker|bool

    - name: girder | service | start
      service:
        name: girder
        state: restarted
      when: is_web|bool

    - name: girder worker | service | start
      service:
        name: girder_worker
        state: restarted
      when: is_worker|bool

### 
###   - name: iGPSe | fetch
###     git_cache:
###       repo: git://github.com/dingh/igpse.git
###       state: present
###     register: igpse
### 
###   - name: variables | igpse | set
###     set_fact:
###       igpse_tag: "{{ igpse.tag }}"
###       igpse_tag_path: "{{ igpse.tag_path }}"
###       igpse_git_work_dir: "{{ igpse.work_path }}"
### 
### - hosts: web
###   user: ubuntu
###   become: true
###   roles:
###   - role: upstart
###     name: igpse
###     user: root
###     description: iGPSe R Shiny App
###     command: >-
###       bash -c "cd /opt/igpse/{{ igpse_tag_path }} && Rscript --no-save
###       --no-restore-history -e
###       'require(shiny) ; shiny::runApp(port=80, host=\"0.0.0.0\")'"
### 
###   post_tasks:
###   - name: igpse | install home | create
###     file:
###       path: /opt/igpse/{{ igpse_tag_path }}
###       state: directory
### 
###   - name: igpse | repo | sync
###     command: >-
###       rsync -avz --exclude=.git
###       "{{ igpse_git_work_dir }}/"
###       "/opt/igpse/{{ igpse_tag_path }}"
### 
###   - name: igpse | service | start
###     service:
###       name: igpse
###       state: started
### 
### 
### 
### 
### 
### 
### 
### 
### 
### 
### 
### 
### 
### 
###           name: sumo
###           region: us-east-1
###           ssh_keys:
###             main: "~/.ssh/sumo.pub"
### 
###           instances:
###             main:
###               ssh_key: main
###               type: t2.medium
###               image: ami-fce3c696
###               volumes: [10]
###               security_groups: ["world_viewable", "ssh", "internal"]
###               ansible_groups: ["sumo", "production", "web"]
###               ip: 52.0.125.243
###             staging:
###               ssh_key: main
###               type: t2.medium
###               image: ami-fce3c696
###               volumes: [10]
###               security_groups: ["kw_viewable", "ssh", "internal"]
###               ansible_groups: ["sumo", "staging", "web"]
###               ip: 52.86.100.86
###             db:
###               ssh_key: main
###               type: t2.medium
###               image: ami-fce3c696
###               volumes: [30]
###               security_groups: ["internal"]
###               ansible_groups: ["sumo", "production", "staging", "db"]
###             worker:
###               ssh_key: main
###               type: t2.medium
###               image: ami-fce3c696
###               volumes: [30]
###               security_groups: ["internal"]
###               ansible_groups: ["sumo", "production", "staging", "worker"]
### 
###           security_groups:
###             internal:
###               - { flow: "sym", proto: "all",
###                   groups: ["internal", "world_viewable", "kw_viewable"] }
### 
###             ssh:
###               - { flow: "in", proto: "tcp", port: 22,
###                   cidr_ip: "66.194.253.0/24" }
### 
###             kw_viewable:
###               - { flow: "in", proto: "tcp", port: [80, 443],
###                   cidr_ip: "66.194.253.0/24" }
### 
###             world_viewable:
###               - { flow: "in", proto: "tcp", port: [80, 443],
###                   cidr_ip: "0.0.0.0/0" }
### 
###             open:
###               - { flow: "sym", proto: "all",
###                   cidr_ip: "0.0.0.0/0" }
### 
### 
### 
### 
### 
###     options:
###       ansible_groups_amend: false

